{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2bedb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoConfig\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from torch.utils.data import ConcatDataset\n",
    "from decimal import Decimal\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from tokenizers import Tokenizer\n",
    "from typing import Dict, List, Optional\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "from typing import Dict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import BartModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42d95cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99fedd88",
   "metadata": {},
   "source": [
    "## Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280a20f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    'hyunwoongko/kobart', sep_token='<sep>'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85f969e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'본성'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WordReplace:\n",
    "    def __init__(self, path=\"../dataset-woorimalsam/similar_words.pkl\"):\n",
    "        # self.tokenizer_encode = lambda x: tokenizer.encode(x)[1:-1]\n",
    "        # self.tokenizer_decode = lambda x: tokenizer.decode(x)\n",
    "        self.tokenizer_encode = lambda x: [y for y in x]\n",
    "        self.tokenizer_decode = lambda x: ''.join(x)\n",
    "        self.pad_token = '_'\n",
    "\n",
    "        df_similar_words = pd.read_pickle(path)\n",
    "        df_similar_words = df_similar_words.sort_index()\n",
    "        \n",
    "        df_similar_words = df_similar_words.map(lambda xs: [self.tokenizer_encode(x) for x in xs])\n",
    "        df_similar_words = df_similar_words.rename(\"similar_words\").to_frame()\n",
    "        df_similar_words[\"word\"] = df_similar_words.index.map(lambda x: self.tokenizer_encode(x))\n",
    "\n",
    "        max_token_len = 10\n",
    "        df_idx = df_similar_words['word'].apply(lambda x: (x+[self.pad_token]*max_token_len)[:max_token_len]).tolist()\n",
    "        df_idx = np.array(df_idx).T.tolist()\n",
    "        df_similar_words.index = df_idx\n",
    "        df_similar_words = df_similar_words.sort_index()\n",
    "        \n",
    "        self.df_similar_words = df_similar_words\n",
    "        \n",
    "    def replace_word(self, word):\n",
    "        tokens = self.tokenizer_encode(word) + [self.pad_token]\n",
    "\n",
    "        df = self.df_similar_words\n",
    "        match = []\n",
    "        for token in tokens:\n",
    "            if token in df.index:\n",
    "                df = df.loc[token]\n",
    "                match.append(token)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if len(match)>0:\n",
    "            residual = df.index.map(lambda x : sum([1 for y in x if y!=self.pad_token]))\n",
    "            df = df[residual<=min(residual)]\n",
    "            candidates = [y for x in df[\"similar_words\"].to_list() for y in x]\n",
    "            # print(df)\n",
    "            return self.tokenizer_decode(candidates[np.random.choice(len(candidates))])\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "wr = WordReplace()\n",
    "wr.replace_word(\"인간성이\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb9c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfee98cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class SplitDataset:\n",
    "    def __init__(self, dataset, ratio=0.8, mode='train'):\n",
    "        self.dataset = dataset\n",
    "        self.ratio = int(ratio*100)\n",
    "        self.mode = mode\n",
    "    \n",
    "    def __len__(self):\n",
    "        ratio = self.ratio\n",
    "        train_len = len(self.dataset) // 100 * ratio + min(len(self.dataset) % 100, ratio)\n",
    "        test_len = len(self.dataset) - train_len\n",
    "        if self.mode =='train':\n",
    "            return train_len\n",
    "        elif self.mode =='test':\n",
    "            return test_len\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        ratio = self.ratio\n",
    "        if self.mode =='train':\n",
    "            return self.dataset.__getitem__(idx//ratio*100+idx%ratio)\n",
    "        elif self.mode=='test':\n",
    "            return self.dataset.__getitem__(idx//(100-ratio)*100+(ratio+idx%(100-ratio)))\n",
    "        \n",
    "\n",
    "class ParallelDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, file_path, max_length = 256, max_history=5, word_noiser=wr.replace_word,\n",
    "        delete_ratio = 0.05, mask_ratio = 0.10, mask_span = 3,\n",
    "        replace_ratio=0.2, diffusion_distance=1,\n",
    "        cache=True\n",
    "    ):\n",
    "        self.load(file_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_encode = lambda x: tokenizer.encode(x)[1:-1]\n",
    "        self.tokenizer_decode = lambda x: tokenizer.decode(x)\n",
    "        self.max_length = max_length\n",
    "        self.max_history = max_history\n",
    "        self.source_enc = {}\n",
    "        self.target_enc = {}\n",
    "        self.tag_bos = [tokenizer.bos_token_id]\n",
    "        self.tag_eos = [tokenizer.eos_token_id]\n",
    "        self.tag_pad = [tokenizer.pad_token_id]\n",
    "        self.tag_sep = [tokenizer.sep_token_id]\n",
    "        self.word_noiser = word_noiser\n",
    "        \n",
    "        self.sample_ratio = 1-delete_ratio\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.mask_span = mask_span\n",
    "        self.replace_ratio = replace_ratio\n",
    "        self.diffusion_distance = diffusion_distance\n",
    "        self.cache = cache\n",
    "        \n",
    "    def load(self, file_path):\n",
    "        target_pkl, source_pkl_list = file_path[\"target_pkl\"], file_path[\"source_pkl_list\"]\n",
    "        with open(target_pkl, \"rb\") as f:\n",
    "            self.data_target = pickle.load(f)\n",
    "        self.data_sources = []\n",
    "        for path in source_pkl_list:\n",
    "            with open(path, \"rb\") as f:\n",
    "                self.data_sources.append(pickle.load(f))\n",
    "        self.keys = list(self.data_target.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys) * len(self.data_sources)\n",
    "    \n",
    "    def _noise_token(self, tokens, sample_ratio = 0.95, mask_ratio = 0.10, mask_span = 3):\n",
    "        # Token Deletion\n",
    "        L = len(tokens)\n",
    "        xmap = np.random.choice(range(L), int(sample_ratio * L), replace=False)\n",
    "        tokens = [tokens[i] for i in sorted(xmap)]\n",
    "        \n",
    "        # Token Masking\n",
    "        for i in np.random.choice(range(len(tokens)), int(mask_ratio*len(tokens)), replace=False):\n",
    "            tokens[i] = tokenizer.mask_token_id\n",
    "        if len(tokens) > mask_span * 3:\n",
    "            a = np.random.randint(len(tokens)-mask_span)\n",
    "            for i in range(a, a+mask_span):\n",
    "                tokens[i] = tokenizer.mask_token_id\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def _noise_text(self, text, replace_ratio=0.2, diffusion_distance=1):\n",
    "        # Token Replace\n",
    "        words = text.split()\n",
    "        for i in np.random.choice(range(len(words)), int(replace_ratio * len(words)), replace=False):\n",
    "            words[i] = self.word_noiser(words[i])\n",
    "\n",
    "        # Token Permutation\n",
    "        Lx = len(words)\n",
    "        xmap = np.argsort( np.arange(0,Lx)+ np.random.normal(0,1+diffusion_distance,Lx) )\n",
    "        words = [words[i] for i in xmap]\n",
    "        \n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def _query(self, idx, noise):\n",
    "        key = self.keys[idx%len(self.keys)]\n",
    "        if (idx not in self.source_enc) or (not self.cache):\n",
    "            self.source_enc[idx] = self.data_sources[idx//len(self.keys)][key].strip()\n",
    "        if idx%len(self.keys) not in self.target_enc:\n",
    "            self.target_enc[idx%len(self.keys)] = self.tokenizer_encode(self.data_target[key].strip())\n",
    "        if noise:\n",
    "            source_enc = self._noise_token(\n",
    "                self.tokenizer_encode(\n",
    "                    self._noise_text(\n",
    "                        self.source_enc[idx],\n",
    "                        replace_ratio=self.replace_ratio, diffusion_distance=self.diffusion_distance\n",
    "                    )\n",
    "                ),\n",
    "                sample_ratio = self.sample_ratio, mask_ratio = self.mask_ratio, mask_span = self.mask_span\n",
    "            )\n",
    "        else:\n",
    "            source_enc = self.tokenizer_encode(self.source_enc[idx])\n",
    "        return source_enc, self.target_enc[idx%len(self.keys)]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        output = []\n",
    "        source_tokens, target_tokens = self._query(idx, noise=True)\n",
    "        source_tokens = self.tag_sep + source_tokens\n",
    "        if (len(source_tokens) == 0) or (len(target_tokens) ==0):\n",
    "            return self.__getitem__(self, (idx+1) % self.__len__())\n",
    "        for i in range(1, self.max_history):\n",
    "            if idx-i<0:\n",
    "                break\n",
    "            s_tokens, _ = self._query(idx-i, noise=False)\n",
    "            if len(self.tag_bos) + len(s_tokens) + len(source_tokens) + len(self.tag_eos) <= self.max_length:\n",
    "                source_tokens = s_tokens + source_tokens\n",
    "            else:\n",
    "                break\n",
    "        return self.make_tensor_from_list(source_tokens, target_tokens)\n",
    "    \n",
    "    def make_tensor_from_list(self, encoder_tokens, decoder_tokens):\n",
    "        encoder_text = self.tokenizer_decode(encoder_tokens)\n",
    "        decoder_text = self.tokenizer_decode(decoder_tokens)\n",
    "        model_inputs = self.tokenizer(encoder_text, max_length=self.max_length, truncation=True)\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(decoder_text, max_length=self.max_length, truncation=True)\n",
    "        model_inputs['labels'] = labels['input_ids']\n",
    "        # del model_inputs['token_type_ids']\n",
    "        return model_inputs\n",
    "    \n",
    "# class CloneDataset(ParallelDataset):\n",
    "        \n",
    "#     def load(self, text_path):\n",
    "#         with open(text_path, \"rt\") as f:\n",
    "#             self.text = f.readlines()\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.text)\n",
    "\n",
    "#     def _query(self, idx):\n",
    "#         dropout_rate = 0.2\n",
    "        \n",
    "#         tokens = self.tokenizer_encode(self.text[idx].strip())\n",
    "#         tokens_partial = [tokens[i] for i in sorted(np.random.choice(len(tokens), int(len(tokens)*(1-dropout_rate)), replace=False))]\n",
    "#         return tokens_partial, tokens\n",
    "    \n",
    "class GeneratedDataset(ParallelDataset):\n",
    "    # GPT generated text\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "def dataset_to_dataloader(ds, num_samples, batch_size = 1):\n",
    "    # Split into training and validation sets\n",
    "    val_size = int(min(10000, len(ds)*0.03))\n",
    "    train_size = len(ds)-val_size\n",
    "\n",
    "    train_set, val_set = random_split(ds, [train_size, val_size])\n",
    "    print(\"train_size :\",int(num_samples),\"/\",train_size)\n",
    "    print(\"val_size   :\",val_size)\n",
    "\n",
    "    train_dataloader = DataLoader(train_set,  sampler = RandomSampler(train_set, num_samples = int(num_samples)), batch_size = batch_size)\n",
    "    validation_dataloader = DataLoader(val_set, sampler = SequentialSampler(val_set), batch_size = batch_size )\n",
    "    return train_dataloader ,validation_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83ef187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Khala_hq = \"../dataset-Khala/ko.pkl\"\n",
    "Khala_lq = \"../dataset-Khala/ko_en_ko.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d64918c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42b6f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_history = 5\n",
    "ratio = 0.99\n",
    "\n",
    "# Khalar parallel\n",
    "dss_train = tuple([\n",
    "    SplitDataset(\n",
    "        ParallelDataset(\n",
    "            {\n",
    "                \"target_pkl\":Khala_hq, \n",
    "                \"source_pkl_list\":[Khala_hq, Khala_lq],\n",
    "            }, max_length = MAX_LEN, max_history=i,\n",
    "            delete_ratio = 0.05, mask_ratio = 0.10, mask_span = 1,\n",
    "            replace_ratio=0.2, diffusion_distance=1.5,\n",
    "        ),\n",
    "        ratio=ratio, mode='train'\n",
    "    )\n",
    "    for i in range(max_history)\n",
    "    ])\n",
    "\n",
    "dss_test = tuple([\n",
    "    SplitDataset(\n",
    "        ParallelDataset(\n",
    "            {\n",
    "                \"target_pkl\":Khala_hq, \n",
    "                \"source_pkl_list\":[Khala_hq, Khala_lq]\n",
    "            }, max_length = MAX_LEN, max_history=i,\n",
    "            delete_ratio = 0.05, mask_ratio = 0.10, mask_span = 1,\n",
    "            replace_ratio=0.2, diffusion_distance=1.5,\n",
    "        ),\n",
    "        ratio=ratio, mode='test'\n",
    "    )\n",
    "    for i in range(max_history)\n",
    "    ])\n",
    "\n",
    "# {k:tokenizer.decode(v) for k,v in dss_train[3][len(dss_train[3])-1].items()}\n",
    "# {k:tokenizer.decode(v) for k,v in dss_test[3][len(dss_test[3])-1].items()}\n",
    "\n",
    "ds_train = ConcatDataset(dss_train)\n",
    "ds_test = ConcatDataset(dss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcfa467b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144540\n",
      "1460\n"
     ]
    }
   ],
   "source": [
    "print(len(ds_train))\n",
    "print(len(ds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0227640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4633cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb1714b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e825cf6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3f0f955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(30001, 768)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(30001, 768)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(30001, 768)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=30001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See also : https://colab.research.google.com/drive/1IHMJHPwoOvAKH7NvyzPjm9cZRSVbLeYR?usp=sharing#scrollTo=LuHj3IJPjrAZ\n",
    "# Create device\n",
    "device = torch.device(\"cuda\")\n",
    "# cfg = AutoConfig.from_pretrained('hyunwoongko/kobart')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('hyunwoongko/kobart')\n",
    "# model.config.update({\n",
    "#     \"encoder_attention_heads\":24,\n",
    "#     \"decoder_attention_heads\":24\n",
    "# })\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# model.resize_position_embeddings\n",
    "# model.cuda()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ec6b39d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq( tokenizer=tokenizer, model=model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eaa786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./bart_model/\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=2, #24, # number of training epochs\n",
    "    per_device_train_batch_size=64, # batch size for training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    eval_steps=500, # Number of update steps between two evaluations.\n",
    "    save_steps=1000, # after # steps model is saved \n",
    "    warmup_steps=300,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=3\n",
    "    )\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=ds_train, \n",
    "    eval_dataset=ds_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e930c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c99f745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 144540\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4518\n",
      "  Number of trainable parameters = 123860736\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4518' max='4518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4518/4518 54:03, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.020100</td>\n",
       "      <td>2.447293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.233300</td>\n",
       "      <td>2.342017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.012400</td>\n",
       "      <td>2.328092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.847000</td>\n",
       "      <td>2.315989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.657100</td>\n",
       "      <td>2.354383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.521400</td>\n",
       "      <td>2.366665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.437800</td>\n",
       "      <td>2.376867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.382000</td>\n",
       "      <td>2.388084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.348100</td>\n",
       "      <td>2.384627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1460\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1460\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./bart_model/checkpoint-1000\n",
      "Configuration saved in ./bart_model/checkpoint-1000/config.json\n",
      "Model weights saved in ./bart_model/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [bart_model/checkpoint-2000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1460\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1460\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./bart_model/checkpoint-2000\n",
      "Configuration saved in ./bart_model/checkpoint-2000/config.json\n",
      "Model weights saved in ./bart_model/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [bart_model/checkpoint-4000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1460\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1460\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./bart_model/checkpoint-3000\n",
      "Configuration saved in ./bart_model/checkpoint-3000/config.json\n",
      "Model weights saved in ./bart_model/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [bart_model/checkpoint-6000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1460\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1460\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./bart_model/checkpoint-4000\n",
      "Configuration saved in ./bart_model/checkpoint-4000/config.json\n",
      "Model weights saved in ./bart_model/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [bart_model/checkpoint-1000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1460\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4518, training_loss=1.8269861026272514, metrics={'train_runtime': 3244.2228, 'train_samples_per_second': 89.106, 'train_steps_per_second': 1.393, 'total_flos': 2.727474315890688e+16, 'train_loss': 1.8269861026272514, 'epoch': 2.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e7fca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32298fd8",
   "metadata": {},
   "source": [
    "## infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02e6522c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    'hyunwoongko/kobart', sep_token='<sep>'\n",
    ")\n",
    "MAX_LEN=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d76499ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlg_pipeline = pipeline('text2text-generation', model=\"./bart_model/checkpoint-4000/\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efde069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(pipe, text, context, num_return_sequences=5, max_length=60):\n",
    "    text = f\"{context}<sep>{text}\"\n",
    "    out = pipe(\n",
    "        text, num_return_sequences=num_return_sequences, max_length=max_length\n",
    "    )\n",
    "    return [x['generated_text'] for x in out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53f9fc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 문장: 나는 우습다고 생각해요. 그 권력에 붙어 있는 사람도 우습고, 권력을 이용하는 사람도 우습고. 나는 그렇게 생각해요.\n",
      "나는 우습다고 생각해요. 그 권력에 붙어 있는 사람도 우습고, 그 권력에 이용하는 사람도 우습고. 나는 그렇게 생각해요.\n"
     ]
    }
   ],
   "source": [
    "# context = \"표절 아닌 것은 세상에 없어요. 다 표절입니다. 난 그렇게 생각해요. (문제 본질은) 문학 권력 투쟁 아니요? 쉽게 말하면 무슨 출판사, 무슨 출판사, 그거 아니요? 그게 문제였지. 뭐, 다른 게 무슨 문제였어요? 표절 아닌 게 세상에 있는 줄 압니까? 우리 말도 다 표절이에요. 엄마 말을 가지고 표절 하는 것 아니에요? 우리가 쓰는 말도.\"\n",
    "context = \"표절 아닌 것은 세상에 없어요. 다 표절입니다. 난 그렇게 생각해요. (문제 본질은) 문학 권력 투쟁 아니요? 쉽게 말하면 무슨 출판사, 무슨 출판사, 그거 아니요? 그게 문제였지. 뭐, 다른 게 무슨 문제였어요? 표절 아닌 게 세상에 있는 줄 압니까? 우리 말도 다 표절이에요. 엄마 말을 가지고 표절 하는 것 아니에요? 우리가 쓰는 말도.\"\n",
    "src_text = \"나는 우습다고 생각해요. 그 권력에 붙어 있는 사람도 우습고, 권력을 이용하는 사람도 우습고. 나는 그렇게 생각해요.\"\n",
    "\n",
    "print(\"입력 문장:\", src_text)\n",
    "print(generate_text(nlg_pipeline, src_text, context, num_return_sequences=1, max_length=1000)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c141c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dbc2728",
   "metadata": {},
   "source": [
    "## infer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dec2b571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    'hyunwoongko/kobart', sep_token='<sep>'\n",
    ")\n",
    "MAX_LEN=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e217eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./bart_model/checkpoint-4000/\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d944f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessor, LogitsProcessorList\n",
    "# avoid_same_thres = 2e-6\n",
    "avoid_same_thres = 1e-5\n",
    "class MyLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, avoid_same_thres):\n",
    "        self.avoid_same_thres = avoid_same_thres\n",
    "        super(MyLogitsProcessor,self).__init__()\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        thres = torch.quantile(scores,1 - self.avoid_same_thres).tolist()\n",
    "        scores[scores>thres]= 2 * thres - scores[scores>thres]\n",
    "        return scores\n",
    "logit_processors = LogitsProcessorList([MyLogitsProcessor(avoid_same_thres=avoid_same_thres)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e684dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.generation_utils import GenerationConfig\n",
    "def infer(context, src_text, \n",
    "          temperature = 1.7, max_len=MAX_LEN, do_sample=True, repetition_penalty=2.0,\n",
    "          num_beams=9, typical_p=0.7, logit_processors=logit_processors,\n",
    "         ):\n",
    "    input_ids = torch.tensor([tokenizer(context + \"<sep>\" + src_text)['input_ids']]).to(device)\n",
    "    gen_ids = model.generate(\n",
    "        inputs = input_ids,\n",
    "        max_length=MAX_LEN,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        use_cache=False,\n",
    "\n",
    "        do_sample=do_sample, \n",
    "        temperature=temperature,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        num_beams=num_beams,\n",
    "        typical_p = typical_p,\n",
    "        renormalize_logits = True,\n",
    "        logits_processor = logit_processors\n",
    "    )\n",
    "    result = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e1bd7033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(context, src_text, n=10, avoid_same_thres=1e-5, temperature=1.7, num_beams=9, typical_p=0.7):\n",
    "    print(f\"avoid_same_thres : {avoid_same_thres}\")\n",
    "    print(f\"temperature : {temperature}\")\n",
    "    print(f\"num_beams : {num_beams}\")\n",
    "    print(f\"typical_p : {typical_p}\")\n",
    "\n",
    "    print(f\"Context : {context}\")\n",
    "    print(f\"Input : {src_text}\")\n",
    "    print()\n",
    "\n",
    "    logit_processors = LogitsProcessorList([MyLogitsProcessor(avoid_same_thres=avoid_same_thres)])\n",
    "    for i in range(n):\n",
    "        output = infer(context, src_text)\n",
    "        print(f\"Output {i} : {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b0da14f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avoid_same_thres : 1e-05\n",
      "temperature : 1.7\n",
      "num_beams : 9\n",
      "typical_p : 0.7\n",
      "Context : 표절 아닌 것은 세상에 없어요. 다 표절입니다. 난 그렇게 생각해요. (문제 본질은) 문학 권력 투쟁 아니요? 쉽게 말하면 무슨 출판사, 무슨 출판사, 그거 아니요? 그게 문제였지. 뭐, 다른 게 무슨 문제였어요? 표절 아닌 게 세상에 있는 줄 압니까? 우리 말도 다 표절이에요. 엄마 말을 가지고 표절 하는 것 아니에요? 우리가 쓰는 말도.\n",
      "Input : 나는 우습다고 생각해요. 그 권력에 붙어 있는 사람도 우습고, 권력을 이용하는 사람도 우습고. 나는 그렇게 생각해요.\n",
      "\n",
      "Output 0 : 나는 그건 또 우습다고 생각해. 그 권력에 붙어 있는 사람도 우습고, 그 권력을 이용하는 사람도 우습고. 나는 그렇게 생각한다, 고 말이네.\n",
      "Output 1 : 나는 생각해봐. 그 권력에 붙어 있는 것도 우습고, 그 권력을 이용하는 사람도 우습고, 나는 그런다고 생각해. 너는 그렇게 생각했재.\n",
      "Output 2 : 나는 그렇게 우스웠다고 생각해. 그 권력에 붙어 있는 것도 우스우고, 그 권력을 이용하는 것도 우스우고, 그랬는데 저는 그렇게 생각해요.”\n",
      "Output 3 : 나는 힘겹고 저러니까 안되는 거죠. 그 힘에 붙어 있는 사람도 우습고, 그 권력을 끌어가는 사람도 안되어 안되는 거지만 나는 그렇게 생각한단 말이야 뭐여.\n",
      "Output 4 : 나는 생각해봐라. 그 권력에 붙어 있는 사람들도 우습고, 그 권력과 결부되어 있는 사람들도 우습고. 나는 그렇게 생각해본다네.\n",
      "Output 5 : 나는 생각해봐, 우습다고. 그 권력에 붙어 있는 것도 우습고, 그 힘을 이용하는 사람도 우습고, 나는 그런 사람이 아니라고 생각해요.\n",
      "Output 6 : 나는 힘에 겹고 그 권력에 붙어 있는 사람도 힘겹고 그 권력에 붙어 있는 사람도 우습고. 나는 그런 생각을 해본 적이 없어요.\n",
      "Output 7 : 나는 힘도 부질없다고 생각해. 권력이 붙든 지위에 있는 자가 우습고, 그렇게 해서 얻어들인 것이 문학이 아닌가 싶기도 하고 그런 거라 생각해.\n",
      "Output 8 : 나는 내가 그렇게 우습다고 생각해요. 그 권력의 편에 서 있는 사람도 우습고, 그 권력의 기반을 만들어가는 사람들도 우습고. 나는 그렇게 생각한다, 고 말하고 싶지 않아요.\n",
      "Output 9 : 나는 생각해봐라. 그 권력에 붙어 있는 것도 우습고, 그 권력을 이용하는 것도 우습고. 나는 그렇게 생각해.\n"
     ]
    }
   ],
   "source": [
    "generate(\n",
    "    context = \"표절 아닌 것은 세상에 없어요. 다 표절입니다. 난 그렇게 생각해요. (문제 본질은) 문학 권력 투쟁 아니요? 쉽게 말하면 무슨 출판사, 무슨 출판사, 그거 아니요? 그게 문제였지. 뭐, 다른 게 무슨 문제였어요? 표절 아닌 게 세상에 있는 줄 압니까? 우리 말도 다 표절이에요. 엄마 말을 가지고 표절 하는 것 아니에요? 우리가 쓰는 말도.\",\n",
    "    src_text = \"나는 우습다고 생각해요. 그 권력에 붙어 있는 사람도 우습고, 권력을 이용하는 사람도 우습고. 나는 그렇게 생각해요.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923b8a18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
